{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb5965b4-d0b7-4fae-9d3b-322a8577acf5",
   "metadata": {},
   "source": [
    "dataset : https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f4ba235-43fa-4f90-9705-d1389232a098",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88993813-f991-4930-8cce-5f8e546af953",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/12 19:31:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BreastCancerClustering\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "098daaeb-a9d7-478a-abf2-a722536bb0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "csv_file_path = \"datasets/BCWD/data.csv\"  # Adjust the file path accordingly\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "810789fa-18f4-41db-bffe-cb72fb30ef76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant features and drop unnecessary columns\n",
    "selected_features = ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']\n",
    "df = df.select(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10f4ce7f-f170-4db7-83fd-bc2e4f203532",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29aa2e71-3ce3-4b7a-b1cf-5d8817ac8ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble features into a single vector column\n",
    "assembler = VectorAssembler(inputCols=selected_features, outputCol=\"features\")\n",
    "df_assembled = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c2532c6-0171-40a3-83de-e8c658f2703d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Scale features to have zero mean and unit standard deviation\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "scaler_model = scaler.fit(df_assembled)\n",
    "df_scaled = scaler_model.transform(df_assembled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dc2b81e-184c-4246-8874-748703b3893b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/12 19:32:11 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n"
     ]
    }
   ],
   "source": [
    "# Train KMeans model\n",
    "kmeans = KMeans(featuresCol='scaled_features', predictionCol='cluster', k=2, seed=42)\n",
    "model = kmeans.fit(df_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfa0ec98-665c-4e73-b106-b1c5b7503330",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(df_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d95e20f-1edf-4fbf-b4c5-2d6f6f8222a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['radius_mean',\n",
       " 'texture_mean',\n",
       " 'perimeter_mean',\n",
       " 'area_mean',\n",
       " 'smoothness_mean',\n",
       " 'compactness_mean',\n",
       " 'concavity_mean',\n",
       " 'concave points_mean',\n",
       " 'symmetry_mean',\n",
       " 'fractal_dimension_mean',\n",
       " 'features',\n",
       " 'scaled_features',\n",
       " 'cluster']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f8f5d1e-49ca-4a70-8aa9-403f7e069917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['radius_mean',\n",
       " 'texture_mean',\n",
       " 'perimeter_mean',\n",
       " 'area_mean',\n",
       " 'smoothness_mean',\n",
       " 'compactness_mean',\n",
       " 'concavity_mean',\n",
       " 'concave points_mean',\n",
       " 'symmetry_mean',\n",
       " 'fractal_dimension_mean',\n",
       " 'features',\n",
       " 'scaled_features']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scaled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4eb3aa50-241e-424d-ad34-de9ad9326354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.6486024129761664\n"
     ]
    }
   ],
   "source": [
    "# Evaluate clustering by computing Silhouette score\n",
    "evaluator = ClusteringEvaluator()\n",
    "silhouette_score = evaluator.evaluate(predictions.withColumnRenamed('cluster','prediction'))\n",
    "print(\"Silhouette Score:\", silhouette_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2168024a-309d-4e7b-a121-aa948dc8bcfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[AMBIGUOUS_REFERENCE] Reference `_1` is ambiguous, could be: [`_1`, `_1`].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m merged_df \u001b[38;5;241m=\u001b[39m df_pca\u001b[38;5;241m.\u001b[39mjoin(predictions, df_pca[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_2\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m predictions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_2\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Convert merged DataFrame to Pandas DataFrame for visualization\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m merged_pd \u001b[38;5;241m=\u001b[39m merged_df\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpca_features\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtoPandas()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Scatter plot of clusters\u001b[39;00m\n\u001b[1;32m     20\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/spark-env/lib/python3.12/site-packages/pyspark/sql/dataframe.py:3227\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   3182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   3183\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   3184\u001b[0m \n\u001b[1;32m   3185\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3225\u001b[0m \u001b[38;5;124;03m    +-----+---+\u001b[39;00m\n\u001b[1;32m   3226\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3227\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jcols(\u001b[38;5;241m*\u001b[39mcols))\n\u001b[1;32m   3228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/anaconda3/envs/spark-env/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/spark-env/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [AMBIGUOUS_REFERENCE] Reference `_1` is ambiguous, could be: [`_1`, `_1`]."
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Perform PCA for dimensionality reduction\n",
    "pca = PCA(k=2, inputCol=\"scaled_features\", outputCol=\"pca_features\")\n",
    "model_pca = pca.fit(df_scaled)\n",
    "df_pca = model_pca.transform(df_scaled)\n",
    "\n",
    "# Add a row index column to both DataFrames\n",
    "df_pca = df_pca.rdd.zipWithIndex().toDF()\n",
    "predictions = predictions.rdd.zipWithIndex().toDF()\n",
    "\n",
    "# Join the DataFrames on the row index column\n",
    "merged_df = df_pca.join(predictions, df_pca[\"_2\"] == predictions[\"_2\"]).drop(\"_2\")\n",
    "\n",
    "# Convert merged DataFrame to Pandas DataFrame for visualization\n",
    "merged_pd = merged_df.select(\"_1\", \"pca_features\").toPandas()\n",
    "\n",
    "# Scatter plot of clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(merged_pd[merged_pd['_1'] == 0]['pca_features'].apply(lambda x: x[0]), \n",
    "            merged_pd[merged_pd['_1'] == 0]['pca_features'].apply(lambda x: x[1]),\n",
    "            color='blue', label='Cluster 0')\n",
    "plt.scatter(merged_pd[merged_pd['_1'] == 1]['pca_features'].apply(lambda x: x[0]), \n",
    "            merged_pd[merged_pd['_1'] == 1]['pca_features'].apply(lambda x: x[1]),\n",
    "            color='red', label='Cluster 1')\n",
    "plt.title(\"Breast Cancer Clustering (PCA)\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c364b5a8-43e2-4e03-9047-6a87e842b4de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
